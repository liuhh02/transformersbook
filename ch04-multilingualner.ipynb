{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset\n\nThe dataset is a subset of the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark called WikiANN or PAN-X.\nConsists of Wikipedia articles in many languages, including the four most commonly spoken languages in Switzerland: German (62.9%), French (22.9%), Italian (8.4%), and English (5.9%).\n\nEach article is annotated with LOC (location), PER (person), and ORG (organization) tags in the “inside-outside-beginning” (IOB2) format. In this format, a B- prefix indicates the beginning of an entity, and consecutive tokens belonging to the same entity are given an I- prefix. An O tag indicates that the token does not belong to any entity. ","metadata":{}},{"cell_type":"code","source":"from datasets import get_dataset_config_names\n\nxtreme_subsets = get_dataset_config_names(\"xtreme\")\nprint(f\"XTREME has {len(xtreme_subsets)} configurations\")","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:26.998611Z","iopub.execute_input":"2022-06-04T21:14:26.999120Z","iopub.status.idle":"2022-06-04T21:14:27.656711Z","shell.execute_reply.started":"2022-06-04T21:14:26.999078Z","shell.execute_reply":"2022-06-04T21:14:27.655393Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"XTREME has 183 configurations\n","output_type":"stream"}]},{"cell_type":"code","source":"panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\npanx_subsets[:3]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:27.658210Z","iopub.execute_input":"2022-06-04T21:14:27.658633Z","iopub.status.idle":"2022-06-04T21:14:27.666629Z","shell.execute_reply.started":"2022-06-04T21:14:27.658595Z","shell.execute_reply":"2022-06-04T21:14:27.665572Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the German corpus\nload_dataset(\"xtreme\", name=\"PAN-X.de\")","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:27.668578Z","iopub.execute_input":"2022-06-04T21:14:27.669416Z","iopub.status.idle":"2022-06-04T21:14:28.408399Z","shell.execute_reply.started":"2022-06-04T21:14:27.669376Z","shell.execute_reply":"2022-06-04T21:14:28.407215Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6b599c3fbe3471e88adde734b680c41"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 20000\n    })\n    validation: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 10000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"To make a realistic Swiss corpus, sample the German (de), French (fr), Italian (it), and English (en) corpora from PAN-X according to their spoken proportions.\nThis will create a language imbalance that is very common in real-world datasets, where acquiring labeled examples in a minority language can be expensive due to the lack of domain experts who are fluent in that language.","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nfrom datasets import DatasetDict\n\nlangs = [\"de\", \"fr\", \"it\", \"en\"]\nfracs = [0.629, 0.229, 0.084, 0.059]\n# Return a DatasetDict if a key doesn't exist\npanx_ch = defaultdict(DatasetDict)\n\nfor lang, frac in zip(langs, fracs):\n    # Load monolingual corpus\n    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n    # Shuffle and downsample each split according to spoken proportion\n    for split in ds:\n        panx_ch[lang][split] = (\n            ds[split]\n            .shuffle(seed=0)\n            .select(range(int(frac * ds[split].num_rows))))","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:28.410361Z","iopub.execute_input":"2022-06-04T21:14:28.411280Z","iopub.status.idle":"2022-06-04T21:14:31.260161Z","shell.execute_reply.started":"2022-06-04T21:14:28.411234Z","shell.execute_reply":"2022-06-04T21:14:31.259097Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65b5f098f4264ffea2312fe81fce6f20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827b6525c34842ad8380bee8d20360e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"092762027ecd48a6b6d545e4a55ed468"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b63c264fea646eca8f28f893fb34ad3"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\npd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n             index=[\"Number of training examples\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:31.261588Z","iopub.execute_input":"2022-06-04T21:14:31.262654Z","iopub.status.idle":"2022-06-04T21:14:31.277413Z","shell.execute_reply.started":"2022-06-04T21:14:31.262610Z","shell.execute_reply":"2022-06-04T21:14:31.275936Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                de    fr    it    en\nNumber of training examples  12580  4580  1680  1180","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>de</th>\n      <th>fr</th>\n      <th>it</th>\n      <th>en</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Number of training examples</th>\n      <td>12580</td>\n      <td>4580</td>\n      <td>1680</td>\n      <td>1180</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Since German makes up the largest proportion of the dataset, we'll use it as a starting point from which to perform zero-shot cross-lingual transfer to French, Italian and English.","metadata":{}},{"cell_type":"code","source":"element = panx_ch[\"de\"][\"train\"][0]\nfor key, value in element.items():\n    print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:31.280541Z","iopub.execute_input":"2022-06-04T21:14:31.282108Z","iopub.status.idle":"2022-06-04T21:14:31.290058Z","shell.execute_reply.started":"2022-06-04T21:14:31.282072Z","shell.execute_reply":"2022-06-04T21:14:31.289115Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.']\nner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\nlangs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n","output_type":"stream"}]},{"cell_type":"code","source":"for key, value in panx_ch[\"de\"][\"train\"].features.items():\n    print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:31.291607Z","iopub.execute_input":"2022-06-04T21:14:31.292441Z","iopub.status.idle":"2022-06-04T21:14:31.303944Z","shell.execute_reply.started":"2022-06-04T21:14:31.292397Z","shell.execute_reply":"2022-06-04T21:14:31.302974Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\nner_tags: Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)\nlangs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\nprint(tags)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:31.305423Z","iopub.execute_input":"2022-06-04T21:14:31.306533Z","iopub.status.idle":"2022-06-04T21:14:31.322319Z","shell.execute_reply.started":"2022-06-04T21:14:31.306467Z","shell.execute_reply":"2022-06-04T21:14:31.321432Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Return a dict with the key corresponding to the new column name and the value as a list of class names\n\ndef create_tag_names(batch):\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n\npanx_de = panx_ch[\"de\"].map(create_tag_names)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:31.323867Z","iopub.execute_input":"2022-06-04T21:14:31.324322Z","iopub.status.idle":"2022-06-04T21:14:37.216005Z","shell.execute_reply.started":"2022-06-04T21:14:31.324279Z","shell.execute_reply":"2022-06-04T21:14:37.214971Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12580 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb27c6210a64f2cab7dfca32b3a0932"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6290 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a627e82bff0643a4bbd7c21537e4fecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6290 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d74d8e50979847d8add4e8bc693a6166"}},"metadata":{}}]},{"cell_type":"code","source":"de_example = panx_de[\"train\"][0]\npd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n['Tokens', 'Tags'])\n\n# The sentence “2,000 Einwohnern an der Danziger Bucht in der polnischen Woiwodschaft Pommern” means \n# “2,000 inhabitants at the Gdansk Bay in the Polish voivodeship of Pomerania” in English","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:37.218265Z","iopub.execute_input":"2022-06-04T21:14:37.218893Z","iopub.status.idle":"2022-06-04T21:14:37.248224Z","shell.execute_reply.started":"2022-06-04T21:14:37.218848Z","shell.execute_reply":"2022-06-04T21:14:37.246984Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"           0           1   2    3         4      5   6    7           8   \\\nTokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \nTags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n\n                  9        10 11  \nTokens  Woiwodschaft  Pommern  .  \nTags           B-LOC    I-LOC  O  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>2.000</td>\n      <td>Einwohnern</td>\n      <td>an</td>\n      <td>der</td>\n      <td>Danziger</td>\n      <td>Bucht</td>\n      <td>in</td>\n      <td>der</td>\n      <td>polnischen</td>\n      <td>Woiwodschaft</td>\n      <td>Pommern</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from collections import Counter\n\nsplit2freqs = defaultdict(Counter)\nfor split, dataset in panx_de.items():\n    for row in dataset[\"ner_tags_str\"]:\n        for tag in row:\n            if tag.startswith(\"B\"):\n                tag_type = tag.split(\"-\")[1]\n                split2freqs[split][tag_type] += 1\npd.DataFrame.from_dict(split2freqs, orient=\"index\")","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:37.249494Z","iopub.execute_input":"2022-06-04T21:14:37.249872Z","iopub.status.idle":"2022-06-04T21:14:37.767013Z","shell.execute_reply.started":"2022-06-04T21:14:37.249829Z","shell.execute_reply":"2022-06-04T21:14:37.766109Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"             LOC   ORG   PER\ntrain       6186  5366  5810\nvalidation  3172  2683  2893\ntest        3180  2573  3071","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LOC</th>\n      <th>ORG</th>\n      <th>PER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>6186</td>\n      <td>5366</td>\n      <td>5810</td>\n    </tr>\n    <tr>\n      <th>validation</th>\n      <td>3172</td>\n      <td>2683</td>\n      <td>2893</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>3180</td>\n      <td>2573</td>\n      <td>3071</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Multilingual transformer models are usually evaluated in three different ways:\n\n1. en: Fine-tune on the English training data and then evaluate on each language’s test set.\n\n2. each: Fine-tune and evaluate on monolingual test data to measure per-language performance.\n\n3. all: Fine-tune on all the training data to evaluate on all on each language’s test set.","metadata":{}},{"cell_type":"markdown","source":"# Tokenization\n\nInstead of using a WordPiece tokenizer, XLM-R uses a tokenizer called SentencePiece that is trained on the raw text of all one hundred languages.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nbert_model_name = \"bert-base-cased\"\nxlmr_model_name = \"xlm-roberta-base\"\nbert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\nxlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:37.768022Z","iopub.execute_input":"2022-06-04T21:14:37.768310Z","iopub.status.idle":"2022-06-04T21:14:45.562923Z","shell.execute_reply.started":"2022-06-04T21:14:37.768283Z","shell.execute_reply":"2022-06-04T21:14:45.561843Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"text = \"Jack Sparrow loves New York!\"\nbert_tokens = bert_tokenizer(text).tokens()\nxlmr_tokens = xlmr_tokenizer(text).tokens()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:45.564336Z","iopub.execute_input":"2022-06-04T21:14:45.564691Z","iopub.status.idle":"2022-06-04T21:14:45.570785Z","shell.execute_reply.started":"2022-06-04T21:14:45.564661Z","shell.execute_reply":"2022-06-04T21:14:45.569760Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"bert_tokens","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:45.571959Z","iopub.execute_input":"2022-06-04T21:14:45.572297Z","iopub.status.idle":"2022-06-04T21:14:45.585901Z","shell.execute_reply.started":"2022-06-04T21:14:45.572268Z","shell.execute_reply":"2022-06-04T21:14:45.584922Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]']"},"metadata":{}}]},{"cell_type":"code","source":"xlmr_tokens","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:14:45.587443Z","iopub.execute_input":"2022-06-04T21:14:45.588095Z","iopub.status.idle":"2022-06-04T21:14:45.601003Z","shell.execute_reply.started":"2022-06-04T21:14:45.588051Z","shell.execute_reply":"2022-06-04T21:14:45.599939Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '!', '</s>']"},"metadata":{}}]},{"cell_type":"markdown","source":"Instead of the \\[CLS\\] and \\[SEP\\] tokens that BERT uses for sentence classification tasks, XLM-R uses \\<s\\> and \\<\\s\\> to denote the start and end of a sequence.","metadata":{}},{"cell_type":"markdown","source":"### Tokenizer Pipeline:\n1. Normalization: apply to a raw string to make it \"cleaner\". Eg. stripping whitespace and removing accented characters, lowercasing\n2. Pretokenization: split text into smaller objects. For most Indo-European languages, strings can typically be split into words on whitespace and punctuation. However, this is not always a trivial and deterministic operation for languages like Chinese, Japanese or Korean. In this case, it might be best to not pretokenize the text and instead use a language-specific library for pretokenization.\n3. Tokenizer model: Apply a subword splitting model on the words. This part of the pipeline *needs to be trained on the corpus*. The role of the model is to split the words into subwords to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens.\n4. Postprocessing: Some additional transformations can be applied on the list of tokens—for instance, adding special tokens at the beginning or end of the input sequence of token indices.","metadata":{}},{"cell_type":"markdown","source":"#### The SentencePiece Tokenizer\nThe SentencePiece tokenizer is based on a type of subword segmentation called Unigram and encodes each input text as a sequence of Unicode characters. This last feature is especially useful for multilingual corpora since it allows SentencePiece to be agnostic about accents, punctuation, and the fact that many languages, like Japanese, do not have whitespace characters.\n\nAnother special feature of SentencePiece is that whitespace is assigned the Unicode symbol U+2581, or the ▁ character, also called the lower one quarter block character. This enables SentencePiece to detokenize a sequence without ambiguities and without relying on language-specific pretokenizers.","metadata":{}},{"cell_type":"markdown","source":"## Creating a Custom Model for Token Classification\n\nBuilding a custom token classification head for XLM-R, using RoBERTa as the base model, but augmented with settings specific to XLM-R.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import XLMRobertaConfig\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n\nclass XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n    config_class = XLMRobertaConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        # Load model body, set add_pooling_layer to False so all hidden states are returned, not only the one with associated with the [CLS] token\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        # Set up token classification head\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        # Load and initialize weights\n        self.init_weights()\n    \n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                labels=None, **kwargs):\n        # Use model body to get encoder representations\n        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n                               token_type_ids=token_type_ids, **kwargs)\n        # Apply classifier to encoder representation\n        sequence_output = self.dropout(outputs[0])\n        logits = self.classifier(sequence_output)\n        # Calculate losses\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        # Return model output object\n        return TokenClassifierOutput(loss=loss, logits=logits,\n                                     hidden_states=outputs.hidden_states,\n                                     attentions=outputs.attentions)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:29:49.609444Z","iopub.execute_input":"2022-06-04T21:29:49.609910Z","iopub.status.idle":"2022-06-04T21:29:49.684150Z","shell.execute_reply.started":"2022-06-04T21:29:49.609875Z","shell.execute_reply":"2022-06-04T21:29:49.683352Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Loading a Custom Model\n\nThe AutoConfig class contains the blueprint of a model’s architecture. When we load a model with AutoModel.from_pretrained(model_ckpt), the configuration file associated with that model is downloaded automatically. \n\nHowever, if we want to modify something like the number of classes or label names, then we can load the configuration first with the parameters we would like to customize.","metadata":{}},{"cell_type":"code","source":"index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n\nfrom transformers import AutoConfig\n\nxlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n                                         num_labels=tags.num_classes,\n                                         id2label=index2tag, label2id=tag2index)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:27:15.583317Z","iopub.execute_input":"2022-06-04T21:27:15.583929Z","iopub.status.idle":"2022-06-04T21:27:15.939177Z","shell.execute_reply.started":"2022-06-04T21:27:15.583883Z","shell.execute_reply":"2022-06-04T21:27:15.938048Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlmr_model = (XLMRobertaForTokenClassification\n              .from_pretrained(xlmr_model_name, config=xlmr_config)\n              .to(device))","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:29:53.544979Z","iopub.execute_input":"2022-06-04T21:29:53.546334Z","iopub.status.idle":"2022-06-04T21:30:50.581625Z","shell.execute_reply.started":"2022-06-04T21:29:53.546215Z","shell.execute_reply":"2022-06-04T21:30:50.580394Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f682082cb81430a959b43b11648afc7"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Quick Test\n\ninput_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\npd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:30:50.583713Z","iopub.execute_input":"2022-06-04T21:30:50.584822Z","iopub.status.idle":"2022-06-04T21:30:50.604278Z","shell.execute_reply.started":"2022-06-04T21:30:50.584777Z","shell.execute_reply":"2022-06-04T21:30:50.603587Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"             0      1      2      3      4  5     6      7   8     9\nTokens     <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York   !  </s>\nInput IDs    0  21763  37456  15555   5161  7  2356   5753  38     2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jack</td>\n      <td>▁Spar</td>\n      <td>row</td>\n      <td>▁love</td>\n      <td>s</td>\n      <td>▁New</td>\n      <td>▁York</td>\n      <td>!</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Input IDs</th>\n      <td>0</td>\n      <td>21763</td>\n      <td>37456</td>\n      <td>15555</td>\n      <td>5161</td>\n      <td>7</td>\n      <td>2356</td>\n      <td>5753</td>\n      <td>38</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"outputs = xlmr_model(input_ids.to(device)).logits\npredictions = torch.argmax(outputs, dim=-1)\nprint(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\nprint(f\"Shape of outputs: {outputs.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:30:50.605199Z","iopub.execute_input":"2022-06-04T21:30:50.605841Z","iopub.status.idle":"2022-06-04T21:30:50.779414Z","shell.execute_reply.started":"2022-06-04T21:30:50.605810Z","shell.execute_reply":"2022-06-04T21:30:50.778261Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Number of tokens in sequence: 10\nShape of outputs: torch.Size([1, 10, 7])\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\npd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:30:54.474903Z","iopub.execute_input":"2022-06-04T21:30:54.475284Z","iopub.status.idle":"2022-06-04T21:30:54.492797Z","shell.execute_reply.started":"2022-06-04T21:30:54.475253Z","shell.execute_reply":"2022-06-04T21:30:54.491891Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"          0      1      2    3      4  5     6      7  8     9\nTokens  <s>  ▁Jack  ▁Spar  row  ▁love  s  ▁New  ▁York  !  </s>\nTags      O      O      O    O      O  O     O      O  O     O","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jack</td>\n      <td>▁Spar</td>\n      <td>row</td>\n      <td>▁love</td>\n      <td>s</td>\n      <td>▁New</td>\n      <td>▁York</td>\n      <td>!</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def tag_text(text, tags, model, tokenizer):\n    # Get tokens with special characters\n    tokens = tokenizer(text).tokens()\n    # Encode the sequence into IDs\n    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n    # Get predictions as distribution over 7 possible classes\n    outputs = model(inputs)[0]\n    # Take argmax to get most likely class per token\n    predictions = torch.argmax(outputs, dim=2)\n    # Convert to DataFrame\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:31:32.517521Z","iopub.execute_input":"2022-06-04T21:31:32.518037Z","iopub.status.idle":"2022-06-04T21:31:32.526650Z","shell.execute_reply.started":"2022-06-04T21:31:32.517995Z","shell.execute_reply":"2022-06-04T21:31:32.525943Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Datasets provides a fast way to tokenize a Dataset object with the map() operation by defining a function with the minimal signature:\n\nfunction(examples: Dict[str, List]) -> Dict[str, List]\n\nwhere examples is equivalent to a slice of a Dataset, e.g., panx_de['train'][:10].\n\nSince the XLM-R tokenizer returns the input IDs for the model’s inputs, we just need to augment this information with the attention mask and the label IDs that encode the information about which token is associated with each NER tag.","metadata":{}},{"cell_type":"code","source":"words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\n\n# tokenize each word and use the is_split_into_words argument to tell the tokenizer that our input sequence has already been split into words\ntokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\ntokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\npd.DataFrame([tokens], index=[\"Tokens\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:34:51.895689Z","iopub.execute_input":"2022-06-04T21:34:51.896151Z","iopub.status.idle":"2022-06-04T21:34:51.970667Z","shell.execute_reply.started":"2022-06-04T21:34:51.896116Z","shell.execute_reply":"2022-06-04T21:34:51.969579Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"         0       1           2  3    4     5     6   7    8      9   ...   15  \\\nTokens  <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...  ▁Wo   \n\n       16   17      18   19    20 21 22 23    24  \nTokens  i  wod  schaft  ▁Po  mmer  n  ▁  .  </s>  \n\n[1 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁2.000</td>\n      <td>▁Einwohner</td>\n      <td>n</td>\n      <td>▁an</td>\n      <td>▁der</td>\n      <td>▁Dan</td>\n      <td>zi</td>\n      <td>ger</td>\n      <td>▁Buch</td>\n      <td>...</td>\n      <td>▁Wo</td>\n      <td>i</td>\n      <td>wod</td>\n      <td>schaft</td>\n      <td>▁Po</td>\n      <td>mmer</td>\n      <td>n</td>\n      <td>▁</td>\n      <td>.</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"word_ids = tokenized_input.word_ids()\npd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:35:25.235288Z","iopub.execute_input":"2022-06-04T21:35:25.235749Z","iopub.status.idle":"2022-06-04T21:35:25.263978Z","shell.execute_reply.started":"2022-06-04T21:35:25.235714Z","shell.execute_reply":"2022-06-04T21:35:25.262593Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"            0       1           2  3    4     5     6   7    8      9   ...  \\\nTokens     <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...   \nWord IDs  None       0           1  1    2     3     4   4    4      5  ...   \n\n           15 16   17      18   19    20  21  22  23    24  \nTokens    ▁Wo  i  wod  schaft  ▁Po  mmer   n   ▁   .  </s>  \nWord IDs    9  9    9       9   10    10  10  11  11  None  \n\n[2 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁2.000</td>\n      <td>▁Einwohner</td>\n      <td>n</td>\n      <td>▁an</td>\n      <td>▁der</td>\n      <td>▁Dan</td>\n      <td>zi</td>\n      <td>ger</td>\n      <td>▁Buch</td>\n      <td>...</td>\n      <td>▁Wo</td>\n      <td>i</td>\n      <td>wod</td>\n      <td>schaft</td>\n      <td>▁Po</td>\n      <td>mmer</td>\n      <td>n</td>\n      <td>▁</td>\n      <td>.</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Word IDs</th>\n      <td>None</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>...</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>10</td>\n      <td>10</td>\n      <td>10</td>\n      <td>11</td>\n      <td>11</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"previous_word_idx = None\nlabel_ids = []\n\nfor word_idx in word_ids:\n    if word_idx is None or word_idx == previous_word_idx:\n        label_ids.append(-100)\n    elif word_idx != previous_word_idx:\n        label_ids.append(labels[word_idx])\n    previous_word_idx = word_idx\n\nlabels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\nindex = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n\npd.DataFrame([tokens, word_ids, label_ids, labels], index=index)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:35:49.502212Z","iopub.execute_input":"2022-06-04T21:35:49.502628Z","iopub.status.idle":"2022-06-04T21:35:49.537974Z","shell.execute_reply.started":"2022-06-04T21:35:49.502595Z","shell.execute_reply":"2022-06-04T21:35:49.536940Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"             0       1           2     3    4     5      6     7     8   \\\nTokens      <s>  ▁2.000  ▁Einwohner     n  ▁an  ▁der   ▁Dan    zi   ger   \nWord IDs   None       0           1     1    2     3      4     4     4   \nLabel IDs  -100       0           0  -100    0     0      5  -100  -100   \nLabels      IGN       O           O   IGN    O     O  B-LOC   IGN   IGN   \n\n              9   ...     15    16    17      18     19    20    21  22    23  \\\nTokens     ▁Buch  ...    ▁Wo     i   wod  schaft    ▁Po  mmer     n   ▁     .   \nWord IDs       5  ...      9     9     9       9     10    10    10  11    11   \nLabel IDs      6  ...      5  -100  -100    -100      6  -100  -100   0  -100   \nLabels     I-LOC  ...  B-LOC   IGN   IGN     IGN  I-LOC   IGN   IGN   O   IGN   \n\n             24  \nTokens     </s>  \nWord IDs   None  \nLabel IDs  -100  \nLabels      IGN  \n\n[4 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁2.000</td>\n      <td>▁Einwohner</td>\n      <td>n</td>\n      <td>▁an</td>\n      <td>▁der</td>\n      <td>▁Dan</td>\n      <td>zi</td>\n      <td>ger</td>\n      <td>▁Buch</td>\n      <td>...</td>\n      <td>▁Wo</td>\n      <td>i</td>\n      <td>wod</td>\n      <td>schaft</td>\n      <td>▁Po</td>\n      <td>mmer</td>\n      <td>n</td>\n      <td>▁</td>\n      <td>.</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Word IDs</th>\n      <td>None</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>...</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>10</td>\n      <td>10</td>\n      <td>10</td>\n      <td>11</td>\n      <td>11</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>Label IDs</th>\n      <td>-100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>6</td>\n      <td>...</td>\n      <td>5</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>6</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>0</td>\n      <td>-100</td>\n      <td>-100</td>\n    </tr>\n    <tr>\n      <th>Labels</th>\n      <td>IGN</td>\n      <td>O</td>\n      <td>O</td>\n      <td>IGN</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>I-LOC</td>\n      <td>...</td>\n      <td>B-LOC</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>I-LOC</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>O</td>\n      <td>IGN</td>\n      <td>IGN</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We choose –100 as the ID to mask subword representations because in PyTorch, the cross-entropy loss class torch.nn.CrossEntropyLoss has an attribute called ignore_index whose value is –100. This index is ignored during training, so we can use it to ignore the tokens associated with consecutive subwords.","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n                                      is_split_into_words=True)\n    labels = []\n    for idx, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None or word_idx == previous_word_idx:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:37:30.832978Z","iopub.execute_input":"2022-06-04T21:37:30.833467Z","iopub.status.idle":"2022-06-04T21:37:30.842231Z","shell.execute_reply.started":"2022-06-04T21:37:30.833430Z","shell.execute_reply":"2022-06-04T21:37:30.841238Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"We can then write a function that iterates through and encodes each split","metadata":{}},{"cell_type":"code","source":"def encode_panx_dataset(corpus):\n    return corpus.map(tokenize_and_align_labels, batched=True,\n                      remove_columns=['langs', 'ner_tags', 'tokens'])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:38:24.088212Z","iopub.execute_input":"2022-06-04T21:38:24.088648Z","iopub.status.idle":"2022-06-04T21:38:24.093437Z","shell.execute_reply.started":"2022-06-04T21:38:24.088614Z","shell.execute_reply":"2022-06-04T21:38:24.092772Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:38:35.216378Z","iopub.execute_input":"2022-06-04T21:38:35.216965Z","iopub.status.idle":"2022-06-04T21:38:39.237298Z","shell.execute_reply.started":"2022-06-04T21:38:35.216924Z","shell.execute_reply":"2022-06-04T21:38:39.236534Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63eec5495aaf4cdabc36eb25447579f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91ee94a5d3c44ddfab329f8683cc30a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c9ebc191adb44119bbd871f9621b12e"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Performance Measures\n\nEvaluating a NER model is similar to evaluating a text classification model, and it is common to report results for precision, recall, and F1-score. The only subtlety is that all words of an entity need to be predicted correctly in order for a prediction to be counted as correct.","metadata":{}},{"cell_type":"code","source":"!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:39:47.164463Z","iopub.execute_input":"2022-06-04T21:39:47.164942Z","iopub.status.idle":"2022-06-04T21:40:04.870647Z","shell.execute_reply.started":"2022-06-04T21:39:47.164906Z","shell.execute_reply":"2022-06-04T21:40:04.869344Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m556.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.21.6)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.0.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=9016a8aa504f4ae5d950629bd9d77c9184258551046c921408720a0a497ac6bc\n  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from seqeval.metrics import classification_report\n\ny_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n          [\"B-PER\", \"I-PER\", \"O\"]]\ny_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n          [\"B-PER\", \"I-PER\", \"O\"]]\nprint(classification_report(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:40:08.376462Z","iopub.execute_input":"2022-06-04T21:40:08.377032Z","iopub.status.idle":"2022-06-04T21:40:09.355789Z","shell.execute_reply.started":"2022-06-04T21:40:08.376986Z","shell.execute_reply":"2022-06-04T21:40:09.354644Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        MISC       0.00      0.00      0.00         1\n         PER       1.00      1.00      1.00         1\n\n   micro avg       0.50      0.50      0.50         2\n   macro avg       0.50      0.50      0.50         2\nweighted avg       0.50      0.50      0.50         2\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\ndef align_predictions(predictions, label_ids):\n    preds = np.argmax(predictions, axis=2)\n    batch_size, seq_len = preds.shape\n    labels_list, preds_list = [], []\n\n    for batch_idx in range(batch_size):\n        example_labels, example_preds = [], []\n        for seq_idx in range(seq_len):\n            # Ignore label IDs = -100\n            if label_ids[batch_idx, seq_idx] != -100:\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n\n        labels_list.append(example_labels)\n        preds_list.append(example_preds)\n\n    return preds_list, labels_list","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:40:55.659360Z","iopub.execute_input":"2022-06-04T21:40:55.660635Z","iopub.status.idle":"2022-06-04T21:40:55.671370Z","shell.execute_reply.started":"2022-06-04T21:40:55.660591Z","shell.execute_reply":"2022-06-04T21:40:55.669836Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### Fine-tuning XLM-RoBERTa","metadata":{}},{"cell_type":"code","source":"# evaluate the model’s predictions on the validation set at the end of every epoch, tweak the weight decay, \n# and set save_steps to a large number to disable checkpointing and thus speed up training.\n\nfrom transformers import TrainingArguments\n\nnum_epochs = 3\nbatch_size = 24\nlogging_steps = len(panx_de_encoded[\"train\"]) // batch_size\nmodel_name = f\"{xlmr_model_name}-finetuned-panx-de\"\ntraining_args = TrainingArguments(\n    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\",\n    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n    logging_steps=logging_steps)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:45:18.437045Z","iopub.execute_input":"2022-06-04T21:45:18.437600Z","iopub.status.idle":"2022-06-04T21:45:18.448557Z","shell.execute_reply.started":"2022-06-04T21:45:18.437555Z","shell.execute_reply":"2022-06-04T21:45:18.447438Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import f1_score\n\ndef compute_metrics(eval_pred):\n    y_pred, y_true = align_predictions(eval_pred.predictions,\n                                       eval_pred.label_ids)\n    return {\"f1\": f1_score(y_true, y_pred)}","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:42:54.358426Z","iopub.execute_input":"2022-06-04T21:42:54.358949Z","iopub.status.idle":"2022-06-04T21:42:54.366180Z","shell.execute_reply.started":"2022-06-04T21:42:54.358909Z","shell.execute_reply":"2022-06-04T21:42:54.364628Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Pad labels along with the inputs to the alrgest sequence length in a batch\n# Padding the labels is necessary because, unlike in a text classification task, the labels are also sequences. \nfrom transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(xlmr_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:43:46.477178Z","iopub.execute_input":"2022-06-04T21:43:46.477679Z","iopub.status.idle":"2022-06-04T21:43:46.592885Z","shell.execute_reply.started":"2022-06-04T21:43:46.477643Z","shell.execute_reply":"2022-06-04T21:43:46.591811Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def model_init():\n    return (XLMRobertaForTokenClassification\n            .from_pretrained(xlmr_model_name, config=xlmr_config)\n            .to(device))","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:44:27.583000Z","iopub.execute_input":"2022-06-04T21:44:27.583590Z","iopub.status.idle":"2022-06-04T21:44:27.590359Z","shell.execute_reply.started":"2022-06-04T21:44:27.583549Z","shell.execute_reply":"2022-06-04T21:44:27.589217Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n                  data_collator=data_collator, compute_metrics=compute_metrics,\n                  train_dataset=panx_de_encoded[\"train\"],\n                  eval_dataset=panx_de_encoded[\"validation\"],\n                  tokenizer=xlmr_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:45:24.835382Z","iopub.execute_input":"2022-06-04T21:45:24.835887Z","iopub.status.idle":"2022-06-04T21:45:28.749010Z","shell.execute_reply.started":"2022-06-04T21:45:24.835842Z","shell.execute_reply":"2022-06-04T21:45:28.747862Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\ntag_text(text_de, tags, trainer.model, xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Error Analysis\n\nExamples where training can fail include:\n\n1. Accidentally masking too many tokens and also masking some of our labels to get a really promising loss drop.\n2. The compute_metrics() function might have a bug that overestimates the true performance.\n3. Including the zero class or O entity in NER as a normal class, which will heavily skew the accuracy and F1-score since it is the majority class by a large margin.","metadata":{}},{"cell_type":"code","source":"from torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    # Convert dict of lists to list of dicts suitable for data collator\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    # Pad inputs and labels and put all tensors on device\n    batch = data_collator(features)\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    with torch.no_grad():\n        # Pass data through model\n        output = trainer.model(input_ids, attention_mask)\n        # logit.size: [batch_size, sequence_length, classes]\n        # Predict class with largest logit value on classes axis\n        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n    # Calculate loss per token after flattening batch dimension with view\n    loss = cross_entropy(output.logits.view(-1, 7),\n                         labels.view(-1), reduction=\"none\")\n    # Unflatten batch dimension and convert to numpy array\n    loss = loss.view(len(input_ids), -1).cpu().numpy()\n\n    return {\"loss\":loss, \"predicted_label\": predicted_label}","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:48:24.854554Z","iopub.execute_input":"2022-06-04T21:48:24.854995Z","iopub.status.idle":"2022-06-04T21:48:24.865646Z","shell.execute_reply.started":"2022-06-04T21:48:24.854965Z","shell.execute_reply":"2022-06-04T21:48:24.864272Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"valid_set = panx_de_encoded[\"validation\"]\nvalid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\ndf = valid_set.to_pandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Examine errors at the token level","metadata":{}},{"cell_type":"code","source":"index2tag[-100] = \"IGN\"\ndf[\"input_tokens\"] = df[\"input_ids\"].apply(\n    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\ndf[\"predicted_label\"] = df[\"predicted_label\"].apply(\n    lambda x: [index2tag[i] for i in x])\ndf[\"labels\"] = df[\"labels\"].apply(\n    lambda x: [index2tag[i] for i in x])\ndf['loss'] = df.apply(\n    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\ndf['predicted_label'] = df.apply(\n    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\ndf.head(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tokens = df.apply(pd.Series.explode)\ndf_tokens = df_tokens.query(\"labels != 'IGN'\")\ndf_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\ndf_tokens.head(7)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group the data by the input tokens and aggregate the losses for each token with the count, mean, and sum.\n# Sort the aggregated data by the sum of the losses and see which tokens have accumulated the most loss in the validation set.\n(\n    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n    .sort_values(by=\"sum\", ascending=False)\n    .reset_index()\n    .round(2)\n    .head(10)\n    .T\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group the label IDs and look at the losses for each class\n(\n    df_tokens.groupby(\"labels\")[[\"loss\"]]\n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)\n    .sort_values(by=\"mean\", ascending=False)\n    .reset_index()\n    .round(2)\n    .T\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the confusion matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:53:46.936126Z","iopub.execute_input":"2022-06-04T21:53:46.936716Z","iopub.status.idle":"2022-06-04T21:53:46.945581Z","shell.execute_reply.started":"2022-06-04T21:53:46.936673Z","shell.execute_reply":"2022-06-04T21:53:46.943804Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n                      tags.names)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Review sequences with high losses","metadata":{}},{"cell_type":"code","source":"def get_samples(df):\n    for _, row in df.iterrows():\n        labels, preds, tokens, losses = [], [], [], []\n        for i, mask in enumerate(row[\"attention_mask\"]):\n            if i not in {0, len(row[\"attention_mask\"])}:\n                labels.append(row[\"labels\"][i])\n                preds.append(row[\"predicted_label\"][i])\n                tokens.append(row[\"input_tokens\"][i])\n                losses.append(f\"{row['loss'][i]:.2f}\")\n        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels,\n                               \"preds\": preds, \"losses\": losses}).T\n        yield df_tmp\n\ndf[\"total_loss\"] = df[\"loss\"].apply(sum)\ndf_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\n\nfor sample in get_samples(df_tmp):\n    display(sample)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Examine sequences with an opening parenthesis, which had a relatively high loss\ndf_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\nfor sample in get_samples(df_tmp):\n    display(sample)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-Lingual Transfer\n\nHaving fine-tuned XLM-R on German, we can evaluate its ability to transfer to other languages via the predict() method of the Trainer","metadata":{}},{"cell_type":"code","source":"def get_f1_score(trainer, dataset):\n    return trainer.predict(dataset).metrics[\"test_f1\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:57:07.574815Z","iopub.execute_input":"2022-06-04T21:57:07.575327Z","iopub.status.idle":"2022-06-04T21:57:07.581070Z","shell.execute_reply.started":"2022-06-04T21:57:07.575290Z","shell.execute_reply":"2022-06-04T21:57:07.580049Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"f1_scores = defaultdict(dict)\nf1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\nprint(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_lang_performance(lang, trainer):\n    panx_ds = encode_panx_dataset(panx_ch[lang])\n    return get_f1_score(trainer, panx_ds[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:57:48.958207Z","iopub.execute_input":"2022-06-04T21:57:48.958747Z","iopub.status.idle":"2022-06-04T21:57:48.965161Z","shell.execute_reply.started":"2022-06-04T21:57:48.958710Z","shell.execute_reply":"2022-06-04T21:57:48.963968Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"f1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tuning on Multiple Languages at Once","metadata":{}},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\ndef concatenate_splits(corpora):\n    multi_corpus = DatasetDict()\n    for split in corpora[0].keys():\n        multi_corpus[split] = concatenate_datasets(\n            [corpus[split] for corpus in corpora]).shuffle(seed=42)\n    return multi_corpus","metadata":{"execution":{"iopub.status.busy":"2022-06-04T22:00:31.026167Z","iopub.execute_input":"2022-06-04T22:00:31.026720Z","iopub.status.idle":"2022-06-04T22:00:31.033187Z","shell.execute_reply.started":"2022-06-04T22:00:31.026681Z","shell.execute_reply":"2022-06-04T22:00:31.031992Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# Concatenate the German and French corpora together\npanx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\n#training_args.push_to_hub = True\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded[\"train\"],\n    eval_dataset=panx_de_fr_encoded[\"validation\"])\n\ntrainer.train()\n#trainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for lang in langs:\n    f1 = evaluate_lang_performance(lang, trainer)\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")","metadata":{},"execution_count":null,"outputs":[]}]}